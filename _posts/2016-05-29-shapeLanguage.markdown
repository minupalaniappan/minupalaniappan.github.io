---
layout: project
title: "Working with OpenCV to build ShapeLanguage"
date:   2016-05-29
categories: projects
summary: "I built a shape recognizer on a broken down touch board at Hack Davis 2016."
link: ""
---
I built a winning hack that was able to recognize shapes drawn on the Synaptics Touch board.

### Source
* [Github code](https://github.com/minupalaniappan/ShapeLanguage)
* [Devpost](http://devpost.com/software/shapelanguage)

### Demo
[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/ca-KuZu4P08/0.jpg)](http://www.youtube.com/watch?v=ca-KuZu4P08)


### How it came about
This is probably my favorite hack of all time. It was the first project that allowed me to interact with hardware and use the python vision library, OpenCV. The project went on to win an award at HackDavis for best use of Synaptics Touch Hardware. The competition lasted for 24 hours.

### Development proccess
The entire application runs right on the terminal; as a result, handling a Node Server or a Python framework wasn't neccesary; all we had to do was write Python and plugin some javascript to listen to the board.

* Thresholds
This program's efficiency is directly correlated to the thresholds read by the touchboard. If a certain drawing reaches the threshold of a given shape, we then determine that the shape to state was the one drawn.

The first step is to determine the threshold colors in which the board is touched or not touched. Remember at each (x,y) coordinate, we compare it against the threshold.

```python
TOUCH_COLOR_THRESHOLD = 200 //the average white (touched)
AVG_COLOR_THRESHOLD = 150 //the average gray
AREA = 64*72 //total size of the board
```

The thresholds are determined by the average values returned for pressure and color. Its done like so...

```python
allReadings = list(chain(*sample['image']))
maxReading = max(allReadings)
minReading = min(allReadings)
avgReading = sum(allReadings) / float(len(allReadings))
return {
	'all': allReadings,
	'max': maxReading,
	'min': minReading,
	'avg': avgReading
}
```
This function determined all the readings on the touch board. If you look at the Git source code, you can see all the output files created from these readings.

The driver for the whole program occured in ```driverFunc()``` and this is the function that derived all readings. It was written like so...

```python
def driverFunc(name):
	f = open(name, 'r')
	samples = [json.loads(line) for line in f.readlines()]
	choordList = []
	for sample in samples:
		readings = fetchReadings(sample)
		colors = fetchColors(readings)
		if(colors['avg'] < AVG_COLOR_THRESHOLD):
			replaceSampleReadingsWithColors(sample, colors['colors'])
			coords = findTouchCoordinates(sample)
			if (sample['sequence'] and coords):
				choordList.append({
					'num': sample['sequence'],
					'coordinates': coords,
					'max': colors['max']}
				)
	fetchAndAddMiddlePointOfState(choordList)
	imgList = []
	for tup in registeredPoints:
		imgList.append(list(tup))
	return (imgList)
```
Here we open up a new pipeline to file generated by the touchboard. Using Python's filestream library, we can implement ```open()``` and read each line using ```json.loads```. From here we can analyze each point to determine which ones were above the ```AVG_COLOR_THRESHOLD```.

So what does this tell us?

It tells us exactly where the user dropped their finger on the touchpad. These boards react to pressure from any given finger, and a result of increased pressure is a change in color. We can determine if the point had an above or below color threshold and determine if there was user-touch there.

We can now recreate the image given the points at which the board was touched.

So, given an image, we can compare it to an actual shape drawn by a graphics tool. Here, I've imported 4 images I created on Sketch.

```python
imgCircle = cv2.imread('imgs/circle.png',0)
imgSquare = cv2.imread('imgs/square.png',0)
imgTriangle = cv2.imread('imgs/triangle.png',0)
imgStar = cv2.imread('imgs/star.png',0)
```

It can currently recognize 4 shapes, since I've plugged in a ```imgCircle```, ```imgSquare```, ```imgTriangle```, ```imgStar```. Furthermore, ```cv2``` is derived from the OpenCV library; hence, I have access to a lot of image comparison functions that allow me to build confidence ratings for each potential match.

In order to transpose and compare the drawn image to the pngs, I have to first add a black and white polyfill to the draw image like so...

```python
cv2.fillPoly(img, np.int32([pts]), (255,255,255))
```

Here ```img``` is the list of points we've compiled from above. The magic of this hack comes from OpenCV's powerful matchShape function.

```python
cmpScores.append(('Circle', cv2.matchShapes(img2, imgCircle, 1,0.0)))
cmpScores.append(('Square', cv2.matchShapes(img2, imgSquare, 1,0.0)))
cmpScores.append(('Triangle', cv2.matchShapes(img2, imgTriangle, 1,0.0)))
cmpScores.append(('Star', cv2.matchShapes(img2, imgStar, 1,0.0)))
```
matchShapes returns a confidence rating which we use to determine certain thresholds.

Given these confidence ratings, we can compare them to the ratings of the actual shapes. Based on how close each confidence rating was, we could determine the shape and vocalize it using Python's ```os```.